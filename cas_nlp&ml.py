# -*- coding: utf-8 -*-
"""CAS_NLP&ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XOQDJ0yr7kOUm8FHvWGAu_tt_479_qAf

# Import dataset and Packages Installation
"""

import pandas as pd
import spacy
import nltk
import copy
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk.corpus import wordnet 
from spacy import displacy
from collections import Counter

#!python -m spacy download en_core_web_lg
import en_core_web_lg
nlp = en_core_web_lg.load()

## for machine learning
from sklearn import preprocessing, model_selection, feature_extraction, feature_selection, metrics, manifold, naive_bayes, pipeline

## for deep learning
from tensorflow.keras import models, layers, preprocessing as kprocessing
from tensorflow.keras import backend as K

## for data
import pandas as pd
import collections
import json
## for plotting
import matplotlib.pyplot as plt
import seaborn as sns
import wordcloud
## for text processing
import re
import nltk
## for language detection
import langdetect 
## for sentiment
from textblob import TextBlob
## for ner
import spacy
## for vectorizer
from sklearn import feature_extraction, manifold
## for word embedding
#import gensim.downloader as gensim_api
## for topic modeling
import gensim
from sklearn.preprocessing import LabelEncoder
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

"""# Prepare data"""

top10_pop = pd.read_csv("top_10_popular.csv")

sum(top10_pop.iloc[:62,2])+sum(top10_pop.iloc[:62,3])

top10_pop.iloc[:62,1:6]

top62 = list(top10_pop.iloc[:62,1])

clean_data = pd.read_csv("clean_data_full.csv")
print(clean_data.shape)
clean_data.columns

clean_data_final=clean_data.drop(list(clean_data.filter(regex='_y$',axis=1).columns), axis=1)

clean_data_final.shape

a = list(clean_data_final.filter(regex='_x$',axis=1).columns)

d = dict()
for i in range(len(a)):
    d[a[i]] =a[i].replace("_x","")
d

clean_data_final=clean_data_final.rename(d, axis='columns')

clean_data_final.columns

clean_data_final.to_csv('clean_data_final.csv')

clean_data_final.shape

dfp = clean_data_final[clean_data_final['neighbourhood'].isin(top62)]
dfp['popular_or_not']='moderate'
dfp.loc[dfp["reviews_per_month"]>=2.65,'popular_or_not'] = 'popular'
dfp.loc[dfp["reviews_per_month"]<=0.16,'popular_or_not'] = 'not popular'
dfp.shape

dfp = pd.concat([dfp.loc[dfp["popular_or_not"]=="popular"],dfp.loc[dfp["popular_or_not"]=="not popular"]])

dfp.shape

dfp.to_csv("popularity_ml_data.csv")

dfp.columns

"""# Feature visualization"""

x, y = "security_deposit", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df_p[y].unique():
    sns.distplot(df_p[df_p[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df_p[df_p[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df_p[y].unique())
ax[1].grid(True)
plt.show()

x, y = "price", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df_p[y].unique():
    sns.distplot(df_p[df_p[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df_p[df_p[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df_p[y].unique())
ax[1].grid(True)
plt.show()

x, y = "host_response_time", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df_p[y].unique():
    sns.distplot(df_p[df_p[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df_p[df_p[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df_p[y].unique())
ax[1].grid(True)
plt.show()

x, y = "cancellation_policy", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df_p[y].unique():
    sns.distplot(df_p[df_p[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df_p[df_p[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df_p[y].unique())
ax[1].grid(True)
plt.show()

x, y = "instant_bookable", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df_p[y].unique():
    sns.distplot(df_p[df_p[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df_p[df_p[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df_p[y].unique())
ax[1].grid(True)
plt.show()

x, y = "distance_nearest_att", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

x, y = "distance_nearest_sub", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

x, y = "number_of_nearest_sub", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

x, y = "number_of_nearest_att", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

x, y = "subways_mentioned", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

x, y = "attractions_mentioned", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

x, y = "adj_keyword_true", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

x, y = "location_keyword_true", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

dfp["name_nearest_sub_encoded"]=LabelEncoder().fit_transform(dfp['name_nearest_sub'])
x, y = "name_nearest_sub_encoded", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

dfp["name_nearest_att_encoded"]=LabelEncoder().fit_transform(dfp['name_nearest_att'])
x, y = "name_nearest_att_encoded", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dfp[y].unique():
    sns.distplot(dfp[dfp[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dfp[dfp[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True,'bw':0.1}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dfp[y].unique())
ax[1].grid(True)
plt.show()

"""# Machine Learning"""

dfp=dfp.fillna('nan')

dfp['popular_or_not_encoded'] = LabelEncoder().fit_transform(dfp['popular_or_not']) # 1=popular, 0=not poopular
dfp['host_response_time_encoded'] = LabelEncoder().fit_transform(dfp['host_response_time'])
dfp['instant_bookable_encoded'] = LabelEncoder().fit_transform(dfp['instant_bookable'])
dfp['cancellation_policy_encoded'] = LabelEncoder().fit_transform(dfp['cancellation_policy'])
dfp['room_type_encoded'] = LabelEncoder().fit_transform(dfp['room_type'])
dfp['name_nearest_sub_encoded'] = LabelEncoder().fit_transform(dfp['name_nearest_sub'])
dfp['name_nearest_att_encoded'] = LabelEncoder().fit_transform(dfp['name_nearest_att'])
imp_col=["name","instant_bookable_encoded","security_deposit","price","cancellation_policy_encoded","popular_or_not_encoded","room_type_encoded","minimum_nights"]#,"host_response_time_encoded"
other_col=["number_of_nearest_sub","number_of_nearest_att","distance_nearest_sub","distance_nearest_att","name_nearest_sub_encoded","name_nearest_att_encoded"]
dfp_all = dfp.loc[:,imp_col+other_col]

dfp_all.columns

#athere
from sklearn.utils import shuffle
dfp_all = shuffle(dfp_all)
dfp_all.shape

df_p_train, df_p_test = model_selection.train_test_split(dfp_all, test_size=0.2, shuffle=True)

y_train = df_p_train["popular_or_not_encoded"].values
y_test = df_p_test["popular_or_not_encoded"].values
X_train = df_p_train.drop(["popular_or_not_encoded","name"], axis=1)
X_test = df_p_test.drop(["popular_or_not_encoded","name"], axis=1)

"""## ExtraTrees"""

from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(X_train, y_train)
predicted = model.predict(X_test)
accuracy_score(y_test, predicted, normalize=True)

cm = metrics.confusion_matrix(y_test, predicted)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)
ax.set(xlabel="Pred", ylabel="True", xticklabels=["not popular","popular"], yticklabels=["not popular","popular"], title="Confusion matrix")
plt.yticks(rotation=0)

print(model.feature_importances_)

forest=model
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(X_train.shape[1]):
    print(list(X_train.columns)[indices[f]], " (%f)" % (importances[indices[f]]))

# Plot the impurity-based feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X_train.shape[1]), importances[indices],
        color="r", yerr=std[indices], align="center")
plt.xticks(range(X_train.shape[1]), indices)
plt.xlim([-1, X_train.shape[1]])


plt.show()

d_feature_eff={}
features = list(X_train.columns)
for i in range(len(features)):
    shuf = list(set(X_test[features[i]]))
    d = {features[i]: shuf}
    df = pd.DataFrame(data=d)
    df["prediction"] = [0] * len(df)
    for j in range(len(shuf)):
        template = np.array([[4,0,0,90,2,1,1,4,0,0.371932, 1.498185,43,6]])
        template[0][i]=shuf[j]
        pred = list(model.predict(template))[0]
        df.prediction[j]=pred
    d_feature_eff[features[i]]=df
        
d_feature_eff

d_feature_eff['security_deposit']

list(X_train.columns)

explainer = lime.lime_tabular.LimeTabularExplainer(X_train.to_numpy(), feature_names=list(X_train.columns), class_names=np.array([0,1]), discretize_continuous=False)

i = np.random.randint(0, X_test.shape[0])
exp = explainer.explain_instance(X_test.to_numpy()[55], model.predict_proba, num_features=13, top_labels=1)

exp.show_in_notebook(show_table=True, show_all=False)

"""## SVM"""

from sklearn import svm
clf = svm.SVC(gamma=0.00006, C=93.)

clf.fit(X_train, y_train)

predicted = clf.predict(X_test)

cm = metrics.confusion_matrix(y_test, predicted)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)
ax.set(xlabel="Pred", ylabel="True", xticklabels=["not popular","popular"], yticklabels=["not popular","popular"], title="Confusion matrix")
plt.yticks(rotation=0)

from sklearn.metrics import accuracy_score
acc_score = accuracy_score(y_test, predicted, normalize=True)
acc_score

"""# NLP

## name nlp topic modeling
"""

df=copy.deepcopy(dfp_all.iloc[:round(4686*0.8)])
df['text']=dfp_all.name

df.text

n_topics = 2

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
vec = TfidfVectorizer(max_features=5000, stop_words="english", max_df=0.95, min_df=2)
features = vec.fit_transform(df.text)

from sklearn.decomposition import NMF
cls = NMF(n_components=n_topics, random_state=30)
cls.fit(features)

# list of unique words found by the vectorizer
feature_names = vec.get_feature_names()

# number of most influencing words to display per topic
n_top_words = 15

for i, topic_vec in enumerate(cls.components_):
    print(i, end=' ')
    # topic_vec.argsort() produces a new array
    # in which word_index with the least score is the
    # first array element and word_index with highest
    # score is the last array element. Then using a
    # fancy indexing [-1: -n_top_words-1:-1], we are
    # slicing the array from its end in such a way that
    # top `n_top_words` word_index with highest scores
    # are returned in desceding order
    for fid in topic_vec.argsort()[-1:-n_top_words-1:-1]:
        print(feature_names[fid], end=' ')
    print()

new_articles =list(dfp_all.name[round(4686*0.8):])
# first transform the text into features using vec
# then pass it to transform of cls
# the result will be a matrix of shape [2, 10]
# then we sort the topic id based on the score using argsort
# and take the last one (with the highest score) for each row using `[:,-1]` indexing
cls.transform(vec.transform(new_articles)).argsort(axis=1)[:,-1]

from sklearn.metrics import accuracy_score
accuracy_score(dfp_all.popular_or_not_encoded[round(4686*0.8):], cls.transform(vec.transform(new_articles)).argsort(axis=1)[:,-1], normalize=True)

"""## popular name word cloud"""

df_popular = pd.concat([df_p.loc[df_p["popular_or_not"]=="popular"]])

df_popular

list(df_popular.name)

# Start with one review:
text = ','.join(list(df_popular.name))

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""# Other explorations"""

def langdetection(x):
    try:
        if x.strip()!="":
            return langdetect.detect(x)
        else:
            return ""
    except:
        return ""
nychouse_data['lang'] = nychouse_data["name"].apply(langdetection)
#nychouse_data['lang'] = nychouse_data["name"].apply(lambda x: langdetect.detect(x) if 
#                                 x.strip() != "" else "")
nychouse_data.head()



nychouse_data.columns

'''
Plot univariate and bivariate distributions.
'''
def utils_plot_distributions(dtf, x, top=None, y=None, bins=None, figsize=(10,5)):
    ## univariate
    if y is None:
        fig, ax = plt.subplots(figsize=figsize)
        fig.suptitle(x, fontsize=15)
        if top is None:
            dtf[x].reset_index().groupby(x).count().sort_values(by="index").plot(kind="barh", legend=False, ax=ax).grid(axis='x')
        else:   
            dtf[x].reset_index().groupby(x).count().sort_values(by="index").tail(top).plot(kind="barh", legend=False, ax=ax).grid(axis='x')
        ax.set(ylabel=None)

    ## bivariate
    else:
        fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=figsize)
        fig.suptitle(x, fontsize=15)
        for i in dtf[y].unique():
            sns.distplot(dtf[dtf[y]==i][x], hist=True, kde=False, bins=bins, hist_kws={"alpha":0.8}, axlabel="", ax=ax[0])
            sns.distplot(dtf[dtf[y]==i][x], hist=False, kde=True, kde_kws={"shade":True, "bw":0.1}, axlabel="", ax=ax[1])
        ax[0].set(title="histogram")
        ax[0].grid(True)
        ax[0].legend(dtf[y].unique())
        ax[1].set(title="density")
        ax[1].grid(True)
    plt.show()

nychouse_data[nychouse_data.lang == "zh-cn"].shape[0]
nychouse_data[nychouse_data.lang == "cy"].shape[0]

utils_plot_distributions(nychouse_data, x="lang", top=20, figsize=(7,3))

abb_raw = pd.read_csv("AB_NYC_2019.csv")
abb_raw.columns

n = 10
top10_lang = nychouse_data['lang'].value_counts()[:n].index.tolist()

df_pop.columns

n = 5
top10_dis = df_pop['neighbourhood_group'].value_counts()[:n].index.tolist()
top10_dis

n = 15
top10_undis = df_unpop['neighbourhood_group'].value_counts()[:n].index.tolist()
top10_undis

import matplotlib.pyplot as plt

x = nychouse_data.lang
y=nychouse_data.review_scores_rating
plt.scatter(x, y)
plt.show()

import math
import stat

abb_raw.calculated_host_listings_count.describe()

import matplotlib.pyplot as plt

x = abb_raw.calculated_host_listings_count
y=nychouse_data.review_scores_rating
plt.scatter(x, y)
plt.show()

nyc_housename = [x for x in nychouse_data["name"].tolist() if x != ""]  
nyc_hosuename_l = [x.lower() for x in nychouse_data["name"].tolist() if x != ""]
nyc_housename[:20]
nyc_hosuename_l[:20]

attractions = pd.read_csv("attraction.csv")
attractions = list(attractions.iloc[:,1])
attractions = [attr.lower() for attr in attractions]
attractions

df = copy.deepcopy(nychouse_data)
df.head()
print(df.columns)
print(df.shape)
sum(df.review_scores_rating==100)
df.describe()

import matplotlib.pyplot as plt

x = df.review_scores_rating
plt.hist(x, bins = 30)
plt.show()

import matplotlib.pyplot as plt

x = df.review_scores_rating
y=df.reviews_per_month
plt.scatter(x, y)
plt.show()

df_pop = df[(df['review_scores_rating']>=98) & (df['reviews_per_month']>=1.58) ]
df_pop.shape

df_unpop = df[(df['review_scores_rating']<=93) & (df['reviews_per_month']<=0.04) ]
df_unpop.shape

'''
Preprocess a string.
:parameter
    :param text: string - name of column containing text
    :param lst_stopwords: list - list of stopwords to remove
    :param flg_stemm: bool - whether stemming is to be applied
    :param flg_lemm: bool - whether lemmitisation is to be applied
:return
    cleaned text
'''
def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):
    ## clean (convert to lowercase and remove punctuations and characters and then strip)
    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())
            
    ## Tokenize (convert from string to list)
    lst_text = text.split()
    ## remove Stopwords
    if lst_stopwords is not None:
        lst_text = [word for word in lst_text if word not in 
                    lst_stopwords]
                
    ## Stemming (remove -ing, -ly, ...)
    if flg_stemm == True:
        ps = nltk.stem.porter.PorterStemmer()
        lst_text = [ps.stem(word) for word in lst_text]
                
    ## Lemmatisation (convert the word into root word)
    if flg_lemm == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_text = [lem.lemmatize(word) for word in lst_text]
            
    ## back to string from list
    text = " ".join(lst_text)
    return text

lst_stopwords = nltk.corpus.stopwords.words("english")

df_pop["name_lemm"] = df_pop["name"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords= lst_stopwords))

df_unpop["name_lemm"] = df_unpop["name"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,lst_stopwords= lst_stopwords))

df_pop['word_count_name'] = df_pop["name"].apply(lambda x: len(str(x).split(" ")))
df_pop['char_count_name'] = df_pop["name"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))
df_pop['sentence_count_name'] = df_pop["name"].apply(lambda x: len(str(x).split(".")))
df_pop['avg_word_length_name'] = df_pop['char_count_name'] / df_pop['word_count_name']
df_pop['avg_sentence_lenght_name'] =df_pop['word_count_name'] / df_pop['sentence_count_name']
df_pop.head()

df_unpop['word_count_name'] = df_unpop["name"].apply(lambda x: len(str(x).split(" ")))
df_unpop['char_count_name'] = df_unpop["name"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))
df_unpop['sentence_count_name'] = df_unpop["name"].apply(lambda x: len(str(x).split(".")))
df_unpop['avg_word_length_name'] = df_unpop['char_count_name'] / df_unpop['word_count_name']
df_unpop['avg_sentence_lenght_name'] =df_unpop['word_count_name'] / df_unpop['sentence_count_name']
df_unpop.head()

#here
df_p['word_count_name'] = df_p["description"].apply(lambda x: len(str(x).split(" ")))
df_p['char_count_name'] = df_p["description"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))
df_p['sentence_count_name'] = df_p["description"].apply(lambda x: len(str(x).split(".")))
df_p['avg_word_length_name'] = df_p['char_count_name'] / df_p['word_count_name']
df_p['avg_sentence_lenght_name'] =df_p['word_count_name'] / df_p['sentence_count_name']

x, y = "char_count_name", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df_p[y].unique():
    sns.distplot(df_p[df_p[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df_p[df_p[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df_p[y].unique())
ax[1].grid(True)
plt.show()

df_p["sentiment_name"] = df_p["description"].apply(lambda x: 
                   TextBlob(x).sentiment.polarity)
df_p.head()

x, y = "sentiment_name", "popular_or_not"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df_p[y].unique():
    sns.distplot(df_p[df_p[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df_p[df_p[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df_p[y].unique())
ax[1].grid(True)
plt.show()

'''
Plot univariate and bivariate distributions.
'''
def utils_plot_distributions(dtf, x, top=None, y=None, bins=None, figsize=(10,5)):
    ## univariate
    if y is None:
        fig, ax = plt.subplots(figsize=figsize)
        fig.suptitle(x, fontsize=15)
        if top is None:
            dtf[x].reset_index().groupby(x).count().sort_values(by="index").plot(kind="barh", legend=False, ax=ax).grid(axis='x')
        else:   
            dtf[x].reset_index().groupby(x).count().sort_values(by="index").tail(top).plot(kind="barh", legend=False, ax=ax).grid(axis='x')
        ax.set(ylabel=None)

    ## bivariate
    else:
        fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=figsize)
        fig.suptitle(x, fontsize=15)
        for i in dtf[y].unique():
            sns.distplot(dtf[dtf[y]==i][x], hist=True, kde=False, bins=bins, hist_kws={"alpha":0.8}, axlabel="", ax=ax[0])
            sns.distplot(dtf[dtf[y]==i][x], hist=False, kde=True, kde_kws={"shade":True, "bw":0.1}, axlabel="", ax=ax[1])
        ax[0].set(title="histogram")
        ax[0].grid(True)
        ax[0].legend(dtf[y].unique())
        ax[1].set(title="density")
        ax[1].grid(True)
    plt.show()

for col in ["word_count_name","word_count_name","sentence_count_name","avg_word_length_name","avg_sentence_lenght_name"]:
    utils_plot_distributions(df_p, x=col, y="popular_or_not", bins=10, figsize=(15,5))

df_p

## tag text and exctract tags into a list
ner = spacy.load("en_core_web_lg")
df_p["tags_name"] = df_p["name"].apply(lambda x: [(tag.text, tag.label_) for tag in ner(x).ents] )
## utils function to count the element of a list
def utils_lst_count(lst):
    dic_counter = collections.Counter()
    for x in lst:
        dic_counter[x] += 1
    dic_counter = collections.OrderedDict( 
                     sorted(dic_counter.items(), 
                     key=lambda x: x[1], reverse=True))
    lst_count = [ {key:value} for key,value in dic_counter.items() ]
    return lst_count

## count tags
df_p["tags_name"] = df_p["tags_name"].apply(lambda x: utils_lst_count(x))

## utils function create new column for each tag category
def utils_ner_features(lst_dics_tuples, tag):
    if len(lst_dics_tuples) > 0:
        tag_type = []
        for dic_tuples in lst_dics_tuples:
            for tuple in dic_tuples:
                type, n = tuple[1], dic_tuples[tuple]
                tag_type = tag_type + [type]*n
                dic_counter = collections.Counter()
                for x in tag_type:
                    dic_counter[x] += 1
        return dic_counter[tag]
    else:
        return 0

## extract features
tags_set = []
for lst in df_p["tags_name"].tolist():
     for dic in lst:
            for k in dic.keys():
                tags_set.append(k[1])
tags_set = list(set(tags_set))
for feature in tags_set:
     df_p["tags__name"+feature] = df_p["tags_name"].apply(lambda x: 
                             utils_ner_features(x, feature))

## print result
df_p.head()

'''
Compute frequency of spacy tags.
'''
def tags_freq(tags, top=30, figsize=(10,5)):   
    tags_list = tags.sum()
    map_lst = list(map(lambda x: list(x.keys())[0], tags_list))
    dtf_tags = pd.DataFrame(map_lst, columns=['tag','type'])
    dtf_tags["count"] = 1
    dtf_tags = dtf_tags.groupby(['type','tag']).count().reset_index().sort_values("count", ascending=False)
    fig, ax = plt.subplots(figsize=figsize)
    fig.suptitle("Top frequent tags", fontsize=12)
    sns.barplot(x="count", y="tag", hue="type", data=dtf_tags.iloc[:top,:], dodge=False, ax=ax)
    ax.set(ylabel=None)
    ax.grid(axis="x")
    plt.show()
    return dtf_tags

for y in df_p["popular_or_not"].unique():
    print("# {}:".format(y))
    common_tags = tags_freq(tags=df_p[df_p["popular_or_not"]==y]["tags_name"], top=10, figsize=(10,3))

# tags bivariate distribution
for x in df_p.filter(like="tags__", axis=1).columns:
    utils_plot_distributions(df_p, x=x, y="popular_or_not", bins=None, figsize=(15,5))

'''
Transform an array of strings into an array of int.
'''
def add_encode_variable(dtf, column):
    dtf[column+"_id"] = dtf[column].factorize(sort=True)[0]
    dic_class_mapping = dict( dtf[[column+"_id",column]].drop_duplicates().sort_values(column+"_id").values )
    return dtf, dic_class_mapping

# not necessary
df_p, dic_y_mapping = add_encode_variable(df_p, "popular_or_not")

print(dic_y_mapping)
df_p[["popular_or_not","popular_or_not_id"]].sample(5)

# Count (classic BoW)
#vectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2), lowercase=False)

# Tf-Idf (advanced variant of BoW)
vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2), lowercase=False)

def df_p_partitioning(dtf, y, test_size=0.3, shuffle=False):
    dtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=test_size, shuffle=shuffle) 
    print("X_train shape:", dtf_train.drop(y, axis=1).shape, "| X_test shape:", dtf_test.drop(y, axis=1).shape)
    print("y:")
    for i in dtf_train["popular_or_not"].value_counts(normalize=True).index:
        print(" ", i, " -->  train:", round(dtf_train["popular_or_not"].value_counts(normalize=True).loc[i], 2),
                          "| test:", round(dtf_test["popular_or_not"].value_counts(normalize=True).loc[i], 2))
    print(dtf_train.shape[1], "features:", dtf_train.drop(y, axis=1).columns.to_list())
    return dtf_train, dtf_test

df_p_train, df_p_test = model_selection.train_test_split(df_p, test_size=0.2, shuffle=True)

df_p_train, df_p_test = model_selection.train_test_split(df_p, test_size=0.2, shuffle=True)
y_train = df_p_train["popular_or_not"].values
y_test = df_p_test["popular_or_not"].values

def fit_bow(corpus, vectorizer=None, vocabulary=None):
    ## vectorizer
    vectorizer = feature_extraction.text.TfidfVectorizer(max_features=None, ngram_range=(1,1), vocabulary=vocabulary) if vectorizer is None else vectorizer
    vectorizer.fit(corpus)
    
    ## sparse matrix
    print("--- creating sparse matrix ---")
    X = vectorizer.transform(corpus)
    print("shape:", X.shape)
    
    ## vocabulary
    print("--- creating vocabulary ---") if vocabulary is None else print("--- used vocabulary ---")
    dic_vocabulary = vectorizer.vocabulary_   #{word:idx for idx, word in enumerate(vectorizer.get_feature_names())}
    print(len(dic_vocabulary), "words")
    
    ## text2tokens
    print("--- tokenization ---")
    tokenizer = vectorizer.build_tokenizer()
    preprocessor = vectorizer.build_preprocessor()
    lst_text2tokens = []
    for text in corpus:
        lst_tokens = [dic_vocabulary[word] for word in tokenizer(preprocessor(text)) if word in dic_vocabulary]
        lst_text2tokens.append(lst_tokens)
    print(len(lst_text2tokens), "texts")
    
    ## plot heatmap
    fig, ax = plt.subplots(figsize=(15,5))
    sns.heatmap(X.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False, ax=ax).set_title('Sparse Matrix Sample')
    plt.show()
    return {"X":X, "lst_text2tokens":lst_text2tokens, "vectorizer":vectorizer, "dic_vocabulary":dic_vocabulary, "X_names":vectorizer.get_feature_names()}

import numpy as np

dic_bow = fit_bow(corpus=df_p_train["name_lemm"], vectorizer=vectorizer, vocabulary=None)

X_train, X_names, vectorizer, dic_vocabulary, lst_text2tokens = dic_bow["X"], dic_bow["X_names"], dic_bow["vectorizer"], dic_bow["dic_vocabulary"], dic_bow["lst_text2tokens"]

f_range = (500, 505)

print("fetures name (from dic_vocabulary):")
print({k:v for k,v in sorted(dic_vocabulary.items(), key=lambda item:item[1], reverse=False)
      if v in np.arange(f_range[0], f_range[1]+1, step=1)})

print("")
print("value of these features (in the sparse matrix) for the first text:")
print(vectorizer.get_feature_names()[f_range[0]:f_range[1]])
print(X_train[0, f_range[0]:f_range[1]].todense())

# check text conversion
i = 0

## list of text: ["I like this", ...]
len_txt = len(df_p["name_lemm"].iloc[i].split())
print("from: ", df_p["name_lemm"].iloc[i], "| len:", len_txt)

## list of token ids: [[1, 2, 3], ...]
len_tokens = len(lst_text2tokens[i])
print("to: ", lst_text2tokens[i], "| len:", len(lst_text2tokens[i]))

## vocabulary: {"I":1, "like":2, "this":3, ...}
print("check: ", df_p["name_lemm"].iloc[i].split()[0], " -- idx in vocabulary -->", 
      dic_vocabulary[df_p["name_lemm"].iloc[i].split()[0]])

## words not in vocabulary?
if len_txt-len_tokens != 0:
    missing_words = [word for word in df_p["name_lemm"].iloc[i].split() if word not in dic_vocabulary.keys()]
    print("!!!", len_txt-len_tokens, "words not in vocabulary:", missing_words)

'''
Perform feature selection using p-values (keep highly correlated features)
:parameter
    :param X: array - like sparse matrix or dtf.values
    :param y: array or dtf - like dtf["y"]
    :param X_names: list - like vetcorizer.get_feature_names()
    :param top: int - ex. 1000 takes the top 1000 features per classes of y. If None takes all those with p-value < 5%.
    :param print_top: int - print top features
:return
    dtf with features and scores
'''
def features_selection(X, y, X_names, top=None, print_top=10):    
    ## selection
    dtf_features = pd.DataFrame()
    for cat in np.unique(y):
        chi2, p = feature_selection.chi2(X, y==cat)
        dtf_features = dtf_features.append(pd.DataFrame({"feature":X_names, "score":1-p, "y":cat}))
    dtf_features = dtf_features.sort_values(["y","score"], ascending=[True,False])
    dtf_features = dtf_features[dtf_features["score"]>0.95] #p-value filter
    if top is not None:
        dtf_features = dtf_features.groupby('y')["y","feature","score"].head(top)
    
    ## print
    print("features selection: from", "{:,.0f}".format(len(X_names)), 
          "to", "{:,.0f}".format(len(dtf_features["feature"].unique())))
    print(" ")
    for cat in np.unique(y):
        print("# {}:".format(cat))
        print("  . selected features:", len(dtf_features[dtf_features["y"]==cat]))
        print("  . top features:", ", ".join(dtf_features[dtf_features["y"]==cat]["feature"].values[:print_top]))
        print(" ")
    return dtf_features["feature"].unique().tolist(), dtf_features

X_names, df_p_selection = features_selection(X_train, df_p_train["popular_or_not"], X_names, top=None, print_top=10)

df_p_selection.sample(5)

# Recreate Vectorizer with the selected vocabulary
dic_bow = fit_bow(corpus=df_p_train["name_lemm"], vocabulary=X_names)

X_train, X_names, vectorizer, dic_vocabulary, lst_text2tokens = dic_bow["X"], dic_bow["X_names"], dic_bow["vectorizer"], dic_bow["dic_vocabulary"], dic_bow["lst_text2tokens"]

'''
Transform a sparse matrix into a dtf with selected features only.
:parameter
    :param X: array - like sparse matrix or dtf.values
    :param dic_vocabulary: dict - {"word":idx}
    :param X_names: list of words - like vetcorizer.get_feature_names()
    :param prefix: str - ex. "x_" -> x_word1, x_word2, ..
'''
def sparse2dtf(X, dic_vocabulary, X_names, prefix=""):
    dtf_X = pd.DataFrame()
    for word in X_names:
        idx = dic_vocabulary[word]
        dtf_X[prefix+word] = np.reshape(X[:,idx].toarray(), newshape=(-1))
    return dtf_X

# Explore the sparse matrix as dtf
df_p_X_train = sparse2dtf(X_train, dic_vocabulary, X_names, prefix="")

df_p_X_train = pd.concat([df_p_train[["name","popular_or_not"]], df_p_X_train.set_index(df_p_train.index)], axis=1)
df_p_X_train.head()

classifier = naive_bayes.MultinomialNB()

X_test = df_p_test["name_lemm"].values

'''
Fits a sklearn classification model.
:parameter
    :param X_train: feature matrix
    :param y_train: array of classes
    :param X_test: raw text
    :param vectorizer: vectorizer object - if None Tf-Idf is used
    :param classifier: model object - if None MultinomialNB is used
:return
    fitted model and predictions
'''
def fit_ml_classif(X_train, y_train, X_test, vectorizer=None, classifier=None): 
    ## model pipeline
    vectorizer = feature_extraction.text.TfidfVectorizer() if vectorizer is None else vectorizer
    classifier = naive_bayes.MultinomialNB() if classifier is None else classifier
    model = pipeline.Pipeline([("vectorizer",vectorizer), ("classifier",classifier)])
    
    ## train
    if vectorizer is None:
        model.fit(X_train, y_train)
    else:
        model["classifier"].fit(X_train, y_train)
    
    ## test
    predicted = model.predict(X_test)
    predicted_prob = model.predict_proba(X_test)
    return model, predicted_prob, predicted

model, predicted_prob, predicted = fit_ml_classif(X_train, y_train, X_test, vectorizer, classifier)

'''
Evaluates a model performance.
:parameter
    :param y_test: array
    :param predicted: array
    :param predicted_prob: array
    :param figsize: tuple - plot setting
'''
def evaluate_multi_classif(y_test, predicted, predicted_prob, figsize=(15,5)):
    classes = np.unique(y_test)
    y_test_array = pd.get_dummies(y_test, drop_first=False).values
    
    ## Accuracy, Precision, Recall
    accuracy = metrics.accuracy_score(y_test, predicted)
   # auc = metrics.roc_auc_score(y_test, predicted, multi_class="ovr")
    print("Accuracy:",  round(accuracy,2))
   # print("Auc:", round(auc,2))
    print("Detail:")
    print(metrics.classification_report(y_test, predicted))
    
    ## Plot confusion matrix
    cm = metrics.confusion_matrix(y_test, predicted)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)
    ax.set(xlabel="Pred", ylabel="True", xticklabels=classes, yticklabels=classes, title="Confusion matrix")
    plt.yticks(rotation=0)

    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=figsize)
    ## Plot roc
    for i in range(len(classes)):
        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i], predicted_prob[:,i])
        ax[0].plot(fpr, tpr, lw=3, label='{0} (area={1:0.2f})'.format(classes[i], metrics.auc(fpr, tpr)))
    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')
    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], xlabel='False Positive Rate', ylabel="True Positive Rate (Recall)", title="Receiver operating characteristic")
    ax[0].legend(loc="lower right")
    ax[0].grid(True)
    
    ## Plot precision-recall curve
    for i in range(len(classes)):
        precision, recall, thresholds = metrics.precision_recall_curve(y_test_array[:,i], predicted_prob[:,i])
        ax[1].plot(recall, precision, lw=3, label='{0} (area={1:0.2f})'.format(classes[i], metrics.auc(recall, precision)))
    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', ylabel="Precision", title="Precision-Recall curve")
    ax[1].legend(loc="best")
    ax[1].grid(True)
    plt.show()

predicted.shape
y_test.shape
predicted_prob.shape

evaluate_multi_classif(y_test, predicted, predicted_prob, figsize=(15,5))



df_pop = nychouse_final.nlargest(6200,['reviews_per_month'])
df_unpop = nychouse_final.nsmallest(6200,['reviews_per_month'])

df_pop = df_1_2y.nlargest(4718,['review_scores_rating'])
df_unpop = df_1_2y.nsmallest(4718,['review_scores_rating'])

def find_Location_Keyword(names,df):
    Location_Keyword = []
    idx = []
    for i in (list(df.index)):
        #print(df.name[i])
        doc = nlp(df.name[i])
        
        if [X.label_ for X in doc.ents] == ['GPE'] or [X.label_ for X in doc.ents] == ['LOC'] or [X.label_ for X in doc.ents] == ['FAC']:
            Location_Keyword.append([X.text for X in doc.ents][0])
            idx.append(i)
           # print(doc.ents)
           # print([(X.text, X.label_) for X in doc.ents])
    return Location_Keyword, idx

df_pop["location_keyword_name_true"] = [0] * len(df_pop)
df_pop["location_keyword_name"] = [''] * len(df_pop)
LK, idx= find_Location_Keyword(nyc_housename,df_pop)

count = 0
for i in idx:
    df_pop.location_keyword_name_true[i] = 1
    df_pop.location_keyword_name[i] = LK[count]
    count += 1

sum(df_pop.location_keyword_name_true)/len(df_pop)

df_unpop["location_keyword_name_true"] = [0] * len(df_unpop)
df_unpop["location_keyword_name"] = [''] * len(df_unpop)
LK, idx= find_Location_Keyword(nyc_housename,df_unpop)

count = 0
for i in idx:
    df_unpop.location_keyword_name_true[i] = 1
    df_unpop.location_keyword_name[i] = LK[count]
    count += 1

sum(df_unpop.location_keyword_name_true)/len(df_unpop)

"""# Feature Engineering

## Location
"""

data_with_name_feature = copy.deepcopy(nychouse_data)

def find_Location_Keyword(names,df):
    Location_Keyword = []
    idx = []
    for i in range(len(names)):
        doc = nlp(names[i])
        if [X.label_ for X in doc.ents] == ['GPE'] or [X.label_ for X in doc.ents] == ['LOC'] or [X.label_ for X in doc.ents] == ['FAC']:
            Location_Keyword.append([X.text for X in doc.ents][0])
            idx.append(i)
           # print(doc.ents)
           # print([(X.text, X.label_) for X in doc.ents])
    return Location_Keyword, idx

data_with_name_feature["location_keyword_true"] = [0] * len(data_with_name_feature)
data_with_name_feature["location_keyword"] = [''] * len(data_with_name_feature)
LK, idx= find_Location_Keyword(nyc_housename,data_with_name_feature)

count = 0
for i in idx:
    data_with_name_feature.location_keyword_true[i] = 1
    data_with_name_feature.location_keyword[i] = LK[count]
    count += 1

"""### GPE LOC FAC"""

def find_GPE(names):
    GPE = []
    idx = []
    for i in range(len(names)):
        doc = nlp(names[i])
        if [X.label_ for X in doc.ents] == ['GPE']:
            GPE.append([X.text for X in doc.ents][0])
            idx.append(i)
            print(doc.ents)
            print([(X.text, X.label_) for X in doc.ents])
    return GPE,idx

GPE,GPE_idx = find_GPE(nyc_housename[:100])
GPE

def find_LOC(names):
    LOC = []
    idx = []
    for i in range(len(names)):
        doc = nlp(names[i])
        if [X.label_ for X in doc.ents] == ['LOC']:
            LOC.append([X.text for X in doc.ents][0])
            idx.append(i)
            print(doc.ents)
            print([(X.text, X.label_) for X in doc.ents])
    return LOC,idx

LOC,LOC_idx = find_LOC(nyc_housename[:100])
LOC

def find_FAC(names):
    FAC = []
    idx = []
    for i in range(len(names)):
        doc = nlp(names[i])
        if [X.label_ for X in doc.ents] == ['FAC']:
            FAC.append([X.text for X in doc.ents][0])
            idx.append(i)
            print(doc.ents)
            print([(X.text, X.label_) for X in doc.ents])
    return FAC,idx

FAC,FAC_idx = find_FAC(nyc_housename[:500])
FAC

"""## ADJ"""

def find_ADJ(names):
    adj = []
    idx = []
    for i in range(len(names)):
        docs = nlp(nyc_housename[i])
        if len([tok for tok in docs if ("ADJ" in tok.pos_)]) > 0 :
            adj.append([tok for tok in docs if ("ADJ" in tok.pos_)])
            idx.append(i)
    return adj,idx

data_with_name_feature["adj_keyword_true"] = [0] * len(data_with_name_feature)
data_with_name_feature["adj_keyword"] = [''] * len(data_with_name_feature)
ADJ,ADJ_idx = find_ADJ(nyc_housename)

count = 0
for i in ADJ_idx:
    data_with_name_feature.adj_keyword_true[i] = 1
    data_with_name_feature.adj_keyword[i] = ADJ[count]
    count += 1

all_adj = {}
for i in range(len(ADJ)):
    for j in range(len(ADJ[i])):
        #print(str(ADJ[i][j]).lower())
        if str(ADJ[i][j]).lower() not in all_adj.keys():
            all_adj[str(ADJ[i][j]).lower()] = 1
        else:
            all_adj[str(ADJ[i][j]).lower()] = all_adj[str(ADJ[i][j]).lower()] + 1

all_adj

top10_adj = dict(Counter(all_adj).most_common(11))
top10_adj_list = list(top10_adj)
top10_adj_list.remove('apt')
top10_adj_list

data_with_name_feature["top10_adj_keyword_rank"] = [0] * len(data_with_name_feature)
data_with_name_feature["top10_adj_keyword"] = [''] * len(data_with_name_feature)
for i in range(len(ADJ)):
    for j in range(len(ADJ[i])):
        if str(ADJ[i][j]).lower()  in top10_adj_list:
            if data_with_name_feature.top10_adj_keyword[ADJ_idx[i]] != '':
                data_with_name_feature.top10_adj_keyword[ADJ_idx[i]] = data_with_name_feature.top10_adj_keyword[ADJ_idx[i]] + ' ' + str(ADJ[i][j]).lower()
            else:
                data_with_name_feature.top10_adj_keyword[ADJ_idx[i]] = str(ADJ[i][j]).lower() 
            if data_with_name_feature.top10_adj_keyword_rank[ADJ_idx[i]] != 0:
                data_with_name_feature.top10_adj_keyword_rank[ADJ_idx[i]]  = str(data_with_name_feature.top10_adj_keyword_rank[ADJ_idx[i]]) + ' ' + str(top10_adj_list.index(str(ADJ[i][j]).lower())+1)
            else:    
                data_with_name_feature.top10_adj_keyword_rank[ADJ_idx[i]]  = str(top10_adj_list.index(str(ADJ[i][j]).lower())+1)

data_with_name_feature

"""## Attraction"""

data_with_name_feature["attractions_mentioned"] = [0] * len(data_with_name_feature)
data_with_name_feature["attractions"] = [''] * len(data_with_name_feature)

attractions_ = copy.deepcopy(attractions)
for attraction in attractions_:
    attraction.replace(" ", "")

nyc_housename_ = copy.deepcopy(nyc_housename)
for name in nyc_housename_:
    name.replace(" ", "")

for k in range(len(nyc_housename_)):
    for i in range(len(attractions_)):
        if attractions_[i] in nyc_housename_[k]:
            print(attractions[i])
            print(nyc_housename[k])
            data_with_name_feature.attractions_mentioned[k] += 1
            data_with_name_feature.attractions[k] = attractions[i]

"""## Subway"""

subway_name = pd.read_csv('subway stop names.csv')
subway_name = list(subway_name.iloc[:,1])
subways = [sub.lower() for sub in subway_name]
subways

data_with_name_feature["subways_mentioned"] = [0] * len(data_with_name_feature)
data_with_name_feature["subways"] = [''] * len(data_with_name_feature)

subways_ = copy.deepcopy(subways)
for subway in subways_:
    subway.replace(" ", "")

nyc_housename_ = copy.deepcopy(nyc_housename)
for name in nyc_housename_:
    name.replace(" ", "")

for k in range(len(nyc_housename_)):
    for i in range(len(subways_)):
        if subways_[i] in nyc_housename_[k]:
            print(subways[i])
            print(nyc_housename[k])
            data_with_name_feature.subways_mentioned[k] = 1
            data_with_name_feature.subways[k] = subways[i]
        elif 'subway' in nyc_housename_[k]:
            print(subways[i])
            print(nyc_housename[k])
            data_with_name_feature.subways_mentioned[k] = 1
            data_with_name_feature.subways[k] = 'subway'
        elif 'metro' in nyc_housename_[k]:
            print(subways[i])
            print(nyc_housename[k])
            data_with_name_feature.subways_mentioned[k] = 1
            data_with_name_feature.subways[k] = 'metro'

data_with_name_feature.to_csv('data_with_name_feature_v4.csv')
from google.colab import files
files.download('data_with_name_feature_v4.csv')

sum(data_with_name_feature.subways_mentioned)

sum(data_with_name_feature.attractions_mentioned)

sum(data_with_name_feature.location_keyword_true)/nychouse_data.shape[0]

sum(data_with_name_feature.adj_keyword_true)/nychouse_data.shape[0]



"""# Sklearn"""

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

df_sklearn = copy.deepcopy(df)
df_sklearn.head()

cat_col = ['neighbourhood','room_type','location_keyword_true','adj_keyword_true','attractions_mentioned','subways_mentioned']
num_col = ['price','minimum_nights']
t = ColumnTransformer(transformers=[
    ('onehot', OneHotEncoder(), cat_col),
    ('scale', StandardScaler(), num_col)
], remainder='passthrough')

# Transform the features
features = t.fit_transform(df_sklearn.iloc[:,1:])
result = df_sklearn.iloc[:,0]

# Train the linear regression model
reg = LinearRegression()
model = reg.fit(features, result)

# Generate a prediction
example = t.transform(pd.DataFrame([{
    'par1': 2, 'par2': 0.33, 'par3': 'no', 'par4': 'red'
}]))
prediction = model.predict(example)
reg_score = reg.score(features, result)
print(prediction, reg_score)





data_with_name_feature.to_csv('data_with_name_feature_v3.csv')
from google.colab import files
files.download('data_with_name_feature_v3.csv')

train = df_.loc[:,['neighbourhood', 'room_type', 'price',
       'minimum_nights', 'location_keyword_true', 'adj_keyword_true',
       'attractions_mentioned', 'subways_mentioned',
       'number of nearest subway stops', 'number of nearest attractions']]
test = df_.loc[:,['reviews_per_month']]

Counter(train['reviews_per_month'])

"""# Pytorch

### Import Modules and Librarys
"""

import pandas as pd
import numpy as np 

from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
import torch.nn.functional as F
import numpy as np
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import sklearn
import statistics 
from tqdm import tqdm
import copy
from random import randint 
import math


from collections import Counter
from sklearn.preprocessing import LabelEncoder

import torch.optim as torch_optim
import torch.nn as nn

from torchvision import models
from datetime import datetime

"""### Input prep"""

final_data = pd.read_csv('abb_final.csv')

final_data.head()

df = final_data.loc[:,['number_of_reviews','reviews_per_month','neighbourhood','room_type','price','minimum_nights','location_keyword_true','adj_keyword_true','attractions_mentioned','subways_mentioned','number of nearest subway stops','number of nearest attractions']]
df.columns

df.dtypes

df_ = copy.deepcopy(df)

df_['neighbourhood'] = LabelEncoder().fit_transform(df_['neighbourhood'])
df_['room_type'] = LabelEncoder().fit_transform(df_['room_type'])

df_.loc[:10,['neighbourhood','room_type']]

cat_col = ['neighbourhood','room_type','location_keyword_true','adj_keyword_true','attractions_mentioned','subways_mentioned']

for col in cat_col:
    df_[col] = df_[col].astype('category')

df_.dtypes

x = np.asarray(df_.reviews_per_month)

level_0_05 = 0
level_05_1 = 0
level_1_15 = 0
level_15_2 = 0
level_2_25 = 0
level_25_3 = 0
level_3_35 = 0
level_35_4 = 0
level_4_45 = 0
level_45_5 = 0
level_5 = 0
count = 0
for i in list(np.asarray(df_.reviews_per_month)):
    if i>0 and i<=0.5:
        level_0_05+=1
        df_.reviews_per_month[count] = '0-0.5'
    elif i>0.5 and i<=1:
        level_05_1+=1
        df_.reviews_per_month[count] = '0.5-1'
    elif i>1 and i<=1.5:
        level_1_15+=1
        df_.reviews_per_month[count] = '1-1.5'
    elif i>1.5 and i<=2:
        level_15_2+=1
        df_.reviews_per_month[count] = '1.5-2'
    elif i>2 and i<=2.5:
        level_2_25+=1
        df_.reviews_per_month[count] = '2-2.5'
    elif i>2.5 and i<=3:
        level_25_3+=1
        df_.reviews_per_month[count] = '2.5-3'
    elif i>3 and i<=3.5:
        level_3_35+=1
        df_.reviews_per_month[count] = '3-3.5'
    elif i>3.5 and i<=4:
        level_35_4+=1
        df_.reviews_per_month[count] = '3.5-4'
    elif i>4 and i<=4.5:
        level_4_45+=1
        df_.reviews_per_month[count] = '4-4.5'
    elif i>4.5 and i<=5:
        level_45_5+=1
        df_.reviews_per_month[count] = '4.5-5'
    elif i>5:
        level_5+=1
        df_.reviews_per_month[count] = '5-above'
    count += 1

level_1=0
level_2=0
level_3=0
level_4=0
level_5=0
level_6=0



count = 0
for i in list(np.asarray(df.reviews_per_month)):
    if i>0 and i<=1:
        level_1+=1
        df_.reviews_per_month[count] = '0-1'
    elif i>1 and i<=2:
        level_2+=1
        df_.reviews_per_month[count] = '1-2'
    elif i>2 and i<=3:
        level_3+=1
        df_.reviews_per_month[count] = '2-3'
    elif i>3 and i<=4:
        level_4+=1
        df_.reviews_per_month[count] = '3-4'
    elif i>4 and i<=5:
        level_5+=1
        df_.reviews_per_month[count] = '4-5'
    elif i>5:
        level_6+=1
        df_.reviews_per_month[count] = '5-above'
    count += 1

level_1_025=0
level_2_05=0
level_3_05_2=0
level_4_2above=0
count = 0
for i in list(np.asarray(df.reviews_per_month)):
    if i>0 and i<=0.25:
        level_1_025+=1
        df_.reviews_per_month[count] = '0-0.25'
    if i>0.25 and i<=0.5:
        level_2_05+=1
        df_.reviews_per_month[count] = '0.25-0.5'
    elif i>0.5 and i<=2:
        level_3_05_2+=1
        df_.reviews_per_month[count] = '0.5-2'
    elif i>2:
        level_4_2above+=1
        df_.reviews_per_month[count] = '2above'
    count += 1

print(level_1_025)
print(level_2_05)
print(level_3_05_2)
print(level_4_2above)

print(level_1)
print(level_2)
print(level_3)
print(level_4)
print(level_5)
print(level_6)

print(level_0_05)
print(level_05_1)
print(level_1_15)
print(level_15_2)
print(level_2_25)
print(level_25_3)
print(level_3_35)
print(level_35_4)
print(level_4_45)
print(level_45_5)
print(level_5)
(level_5+level_45_5+level_4_45+level_35_4+level_3_35+level_25_3+level_2_25)/len(list(np.asarray(df_.reviews_per_month)))

"""### Training"""

len(list(np.asarray(df_.reviews_per_month)))*0.276843467011643

Y = LabelEncoder().fit_transform(df_.iloc[:,0])

#sanity check to see numbers match and matching with previous counter to create target dictionary
print(Counter(df_.iloc[:,0]))
print(Counter(df_.iloc[:,0]))
target_dict = {
    '0-0.5' : 0,
    '0.5-1': 1,
    '1-1.5' : 2,
    '1.5-2': 3,
    '2-2.5' : 4,
    '2.5-3': 5,
    '3-3.5' : 6,
    '3.5-4': 7,
    '4-4.5' : 8,
    '4.5-5': 9,
    '5-above' : 10,

}

Y

X_train, X_test, y_train, y_test = train_test_split(df_.iloc[:,1:], df_.iloc[:,0], test_size=0.20, random_state=42)

X_train.head()

y_train.head()

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

X_train

X_val=X_test
y_val=y_test

df_cat = df_.loc[:,cat_col]
embedded_cols = {n: len(col.cat.categories) for n,col in df_cat.items() if len(col.cat.categories) > 2}
embedded_cols

embedded_col_names = embedded_cols.keys()
embedded_col_names

embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]
embedding_sizes

class ShelterOutcomeDataset(Dataset):
    def __init__(self, X, Y, embedded_col_names):
        X = X.copy()
        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns
        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns
        self.y = Y
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        return self.X1[idx], self.X2[idx], self.y[idx]

train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)
valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)

class ShelterOutcomeModel(nn.Module):
    def __init__(self, embedding_sizes, n_cont):
        super().__init__()
        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])
        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined
        self.n_emb, self.n_cont = n_emb, n_cont
        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)
        self.lin2 = nn.Linear(200, 70)
        self.lin3 = nn.Linear(70, 5)
        self.bn1 = nn.BatchNorm1d(self.n_cont)
        self.bn2 = nn.BatchNorm1d(200)
        self.bn3 = nn.BatchNorm1d(70)
        self.emb_drop = nn.Dropout(0.6)
        self.drops = nn.Dropout(0.3)
        

    def forward(self, x_cat, x_cont):
        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]
        x = torch.cat(x, 1)
        x = self.emb_drop(x)
        x2 = self.bn1(x_cont)
        x = torch.cat([x, x2], 1)
        x = F.relu(self.lin1(x))
        x = self.drops(x)
        x = self.bn2(x)
        x = F.relu(self.lin2(x))
        x = self.drops(x)
        x = self.bn3(x)
        x = self.lin3(x)
        return x

model = ShelterOutcomeModel(embedding_sizes, 1)
to_device(model, device)

def train_model(model, optim, train_dl):
    model.train()
    total = 0
    sum_loss = 0
    for x1, x2, y in train_dl:
        batch = y.shape[0]
        output = model(x1, x2)
        loss = F.cross_entropy(output, y)   
        optim.zero_grad()
        loss.backward()
        optim.step()
        total += batch
        sum_loss += batch*(loss.item())
    return sum_loss/total

def val_loss(model, valid_dl):
    model.eval()
    total = 0
    sum_loss = 0
    correct = 0
    for x1, x2, y in valid_dl:
        current_batch_size = y.shape[0]
        out = model(x1, x2)
        loss = F.cross_entropy(out, y)
        sum_loss += current_batch_size*(loss.item())
        total += current_batch_size
        pred = torch.max(out, 1)[1]
        correct += (pred == y).float().sum().item()
    print("valid loss %.3f and accuracy %.3f" % (sum_loss/total, correct/total))
    return sum_loss/total, correct/total

def train_loop(model, epochs, lr=0.01, wd=0.0):
    optim = get_optimizer(model, lr = lr, wd = wd)
    for i in range(epochs): 
        loss = train_model(model, optim, train_dl)
        print("training loss: ", loss)
        val_loss(model, valid_dl)



batch_size = 1000
train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)
valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)

train_dl = DeviceDataLoader(train_dl, device)
valid_dl = DeviceDataLoader(valid_dl, device)





class CustomDataset(Dataset):
    def __init__(self, X, y, embedded_col_names):
        X = X.copy()#copy.deepcopy(X) #
        self.X_cat = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns
        self.X_num = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns
        self.y = y
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        return self.X_cat[idx], self.X_num[idx], self.y[idx]

#creating train and test datasets
training_set = CustomDataset(X_train, y_train, embedded_col_names)
testing_set = CustomDataset(X_test, y_test, embedded_col_names)
batch_size = 500
train_loader = DataLoader(
    dataset=training_set,
    batch_size=batch_size, 
    shuffle=True)
test_loader = DataLoader(
    dataset=testing_set,
    batch_size=batch_size, 
    shuffle=False)

def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

class DeviceDataLoader():
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device
        
    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl: 
            yield to_device(b, self.device)

    def __len__(self):
        """Number of batches"""
        return len(self.dl)

device = get_default_device()
device



class Model(nn.Module):
    def __init__(self, embedding_sizes, n_cont):
        super().__init__()
        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])
        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined
        self.n_emb, self.n_cont = n_emb, n_cont
        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)
        self.lin2 = nn.Linear(200, 70)
        self.lin3 = nn.Linear(70, 5)
        self.bn1 = nn.BatchNorm1d(self.n_cont)
        self.bn2 = nn.BatchNorm1d(200)
        self.bn3 = nn.BatchNorm1d(70)
        self.emb_drop = nn.Dropout(0.05)
        self.drops = nn.Dropout(0.03)
        

    def forward(self, x_cat, x_cont):
        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]
        x = torch.cat(x, 1)
        x = self.emb_drop(x)
        x2 = self.bn1(x_cont)
        x = torch.cat([x, x2], 1)
        x = F.relu(self.lin1(x))
        x = self.drops(x)
        x = self.bn2(x)
        x = F.relu(self.lin2(x))
        x = self.drops(x)
        x = self.bn3(x)
        x = self.lin3(x)
        return x

model = Model(embedding_sizes, 1)
to_device(model, device)

def get_optimizer(model, lr = 0.001, wd = 0.0):
    parameters = filter(lambda p: p.requires_grad, model.parameters())
    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)
    return optim

def train_model(model, optim, train_loader):
    model.train()
    total = 0
    sum_loss = 0
    for x1, x2, y in  enumerate(train_loader):
        batch = y.shape[0]
        output = model(x1, x2)
        loss = F.cross_entropy(output, y)   
        optim.zero_grad()
        loss.backward()
        optim.step()
        total += batch
        sum_loss += batch*(loss.item())
    return sum_loss/total

def val_loss(model, test_loader):
    model.eval()
    total = 0
    sum_loss = 0
    correct = 0
    for x1, x2, y in test_loader:
        current_batch_size = y.shape[0]
        out = model(x1, x2)
        loss = F.cross_entropy(out, y)
        sum_loss += current_batch_size*(loss.item())
        total += current_batch_size
        pred = torch.max(out, 1)[1]
        correct += (pred == y).float().sum().item()
    print("valid loss %.3f and accuracy %.3f" % (sum_loss/total, correct/total))
    return sum_loss/total, correct/total

def train_loop(model, epochs, lr=0.01, wd=0.0):
    optim = get_optimizer(model, lr = lr, wd = wd)
    for i in range(epochs): 
        loss = train_model(model, optim, train_loader)
        print("training loss: ", loss)
        val_loss(model, test_loader)

for data in train_loader:
    print(data)

optim = get_optimizer(model, lr = 0.05, wd = 0.00001)
train_model(model, optim, train_loader)

train_loop(model, epochs=8, lr=0.05, wd=0.00001)

X_train = np.array(traindf.iloc[:,4:])
y_train = np.array(traindf.iloc[:,3]).reshape(-1)

# #check for device available
# from tensorflow.python.client import device_lib
# device_lib.list_local_devices()



X_test = np.array(testdf.iloc[:,4:])
y_test = np.array(testdf.iloc[:,3]).reshape(-1)

X_train = torch.tensor(X_train, device="cuda").type(torch.float)
y_train = torch.tensor(y_train, device="cuda").type(torch.float)
X_test = torch.tensor(X_test, device="cuda").type(torch.float)
y_test = torch.tensor(y_test, device="cuda").type(torch.float)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

X_mean = X_train.mean(dim=0)
X_std = X_train.std(dim=0)
y_mean = y_train.mean(dim=0)
y_std = y_train.std(dim=0)
X_mean.shape, X_std.shape, y_mean.shape, y_std.shape

print(X_mean)
print(X_std)
print(y_mean)
print(y_std)

class CustomDataset(Dataset):
    def __init__(self, X, y, transform=None, target_transform=None):
        super().__init__()
        self.X = X
        self.y = y
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        X = self.X[idx]
        y = self.y[idx]

        if self.transform is not None:
            X = self.transform(X)

        if self.target_transform is not None:
            y = self.target_transform(y)
        return X, y

training_set = CustomDataset(X_train, 
                             y_train, 
                             transform=lambda X: ((X - X_mean)/X_std), 
                             target_transform=lambda y: ((y - y_mean)/y_std)
                          )

"""# Pop vs unpop exploration"""

import pandas as pd
import numpy as np
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import Dataset, DataLoader
import torch.optim as torch_optim
import torch.nn as nn
import torch.nn.functional as F
#from torchvision import models
from datetime import datetime

import pandas as pd
import numpy as np 

from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
import torch.nn.functional as F
import numpy as np
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import sklearn
import statistics 
from tqdm import tqdm
import copy
from random import randint 
import math


from collections import Counter
from sklearn.preprocessing import LabelEncoder

import torch.optim as torch_optim
import torch.nn as nn

#from torchvision import models
from datetime import datetime

df_pop.columns

df_pop['name_nearest_sub'] = LabelEncoder().fit_transform(df_pop['name_nearest_sub'])
df_pop['name_nearest_att'] = LabelEncoder().fit_transform(df_pop['name_nearest_att'])

df_unpop['name_nearest_sub'] = LabelEncoder().fit_transform(df_unpop['name_nearest_sub'])
df_unpop['name_nearest_att'] = LabelEncoder().fit_transform(df_unpop['name_nearest_att'])

df_pop['name_nearest_sub']

print(sum(df_pop.subways_mentioned))
l = list(df_pop['number_of_nearest_sub'])
l.sort()
print(l)
print(l.count(4))
print(l.count(3))
print(l.count(2))
print(l.count(1))
print(l.count(0))

print(sum(df_unpop.subways_mentioned))
lu = list(df_unpop['number_of_nearest_sub'])
lu.sort()
print(lu)
print(lu.count(4))
print(lu.count(3))
print(lu.count(2))
print(lu.count(1))
print(lu.count(0))

df_pop.columns

import matplotlib.pyplot as plt

x = df_pop.name_nearest_sub
plt.hist(x, bins = 30)
plt.show()

df_pop.name_nearest_sub.describe()

n = 10
top10_sub = df_pop['name_nearest_sub'].value_counts()[:n].index.tolist()
top10_sub

n = 55
top10_att = df_pop['name_nearest_att'].value_counts()[:n].index.tolist()
top10_att

n = 55
top10_unatt = df_unpop['name_nearest_att'].value_counts()[:n].index.tolist()
top10_unatt

n = 10
top10_unsub = df_unpop['name_nearest_sub'].value_counts()[:n].index.tolist()
top10_unsub

import matplotlib.pyplot as plt

x = df_pop.distance_nearest_sub
y=df_pop.name_nearest_sub
plt.scatter(x, y)
plt.show()

import matplotlib.pyplot as plt

x = df_unpop.distance_nearest_sub
y=df_unpop.name_nearest_sub
plt.scatter(x, y)
plt.show()

import matplotlib.pyplot as plt

x = df_pop.distance_nearest_sub
plt.hist(x, bins = 30)
plt.show()

df_pop.name_nearest_sub

import matplotlib.pyplot as plt

x = df_unpop.distance_nearest_att
plt.hist(x, bins = 30)
plt.show()

print(sum(df_pop.attractions_mentioned))
l = list(df_pop['number of nearest attractions'])
l.sort()
print(l)
print(l.count(4))
print(l.count(3))
print(l.count(2))
print(l.count(1))
print(l.count(0))

print(sum(df_unpop.attractions_mentioned))
lu = list(df_unpop['number of nearest attractions'])
lu.sort()
print(lu)
print(lu.count(4))
print(lu.count(3))
print(lu.count(2))
print(lu.count(1))
print(lu.count(0))

print(sum(df_pop.adj_keyword_true))
print(sum(df_unpop.adj_keyword_true))

print(sum(df_pop.location_keyword_true))
print(sum(df_unpop.location_keyword_true))

print(sum(df_pop.minimum_nights))
l = list(df_pop['minimum_nights'])
l.sort()
print(l)
print(l.count(4))
print(l.count(3))
print(l.count(2))
print(l.count(1))
print(l.count(0))

print(sum(df_unpop.minimum_nights))
l = list(df_unpop['minimum_nights'])
l.sort()
print(l)
print(l.count(4))
print(l.count(3))
print(l.count(2))
print(l.count(1))
print(l.count(0))

#print(sum(df_pop.top10_adj_keyword_rank))
l = list(df_pop['top10_adj_keyword_rank'])
l.sort()
r1=0
r2=2
r3=0
r4=0
r5=0
r6=0
r7=0
r8=0
r9=0
r10=0
for i in df_pop.top10_adj_keyword_rank:

    if '1' in i:
        r1+=1
    if '2' in i:
        r2+=1
    if '3' in i:
        r3+=1
    if '4' in i:
        r4+=1
    if '5' in i:
        r5+=1
    if '6' in i:
        r6+=1
    if '7' in i:
        r7+=1
    if '8' in i:
        r8+=1
    if '9' in i:
        r9+=1
    if '10' in i:
        r10+=1
print(r1)
print(r2)
print(r3)
print(r4)
print(r5)
print(r6)
print(r7)
print(r8)
print(r9)
print(r10)

#print(sum(df_pop.top10_adj_keyword_rank))
l = list(df_unpop['top10_adj_keyword_rank'])
l.sort()
r1=0
r2=2
r3=0
r4=0
r5=0
r6=0
r7=0
r8=0
r9=0
r10=0
for i in df_unpop.top10_adj_keyword_rank:

    if '1' in i:
        r1+=1
    if '2' in i:
        r2+=1
    if '3' in i:
        r3+=1
    if '4' in i:
        r4+=1
    if '5' in i:
        r5+=1
    if '6' in i:
        r6+=1
    if '7' in i:
        r7+=1
    if '8' in i:
        r8+=1
    if '9' in i:
        r9+=1
    if '10' in i:
        r10+=1
print(r1)
print(r2)
print(r3)
print(r4)
print(r5)
print(r6)
print(r7)
print(r8)
print(r9)
print(r10)

df_pop.neighbourhood
pop = pd.DataFrame(df_pop.neighbourhood.value_counts())

df_unpop.neighbourhood
temp = pd.DataFrame(df_unpop.neighbourhood.value_counts())

temp.to_excel('unpop.xlsx')
from google.colab import files
files.download('unpop.xlsx')

"""### Comparison"""

df_2 = copy.deepcopy(df)

df_2['month']=df_2.number_of_reviews/df_2.reviews_per_month

df_2

df_1_2y = df_2.loc[(df_2['month'] >= 12)]

df_1_2y=df_1_2y.sort_values(by=['reviews_per_month'])
df_1_2y

df_pop = df_1_2y.nlargest(4718,['reviews_per_month'])
df_unpop = df_1_2y.nsmallest(4718,['reviews_per_month'])

df_pop

df_unpop

df_2.loc[(df_2['month'] >= 11) & (df_2['month'] <= 12)]

df_pop = df_2.nlargest(6957,['reviews_per_month'])
df_unpop = df_2.nsmallest(6957,['reviews_per_month'])

df_pop.reviews_per_month = 'popular'

df_unpop.reviews_per_month = 'unpopular'

df_popornot = pd.concat([df_pop,df_unpop])
df_popornot

from sklearn.utils import shuffle
df_popornot = shuffle(df_popornot)

df_01 = df_[df_['reviews_per_month'] == '0-1']
df_02 = df_[df_['reviews_per_month'] == '1-2']
df_03 = df_[df_['reviews_per_month'] == '2-3']
df_04 = df_[df_['reviews_per_month'] == '3-4']
df_05 = df_[df_['reviews_per_month'] == '4-5']
df_06 = df_[df_['reviews_per_month'] == '5-above']

print(len(df_01))
print(len(df_02))
print(len(df_03))
print(len(df_04))
print(len(df_05))
print(len(df_06))
print(len(df_))
19333/34785
34785*0.2

print(sum(df_01["subways_mentioned"])/len(df_01))
print(sum(df_02["subways_mentioned"])/len(df_02))
print(sum(df_03["subways_mentioned"])/len(df_03))
print(sum(df_04["subways_mentioned"])/len(df_04))
print(sum(df_05["subways_mentioned"])/len(df_05))
print(sum(df_06["subways_mentioned"])/len(df_06))

print(sum(df_01["attractions_mentioned"])/len(df_01))
print(sum(df_02["attractions_mentioned"])/len(df_02))
print(sum(df_03["attractions_mentioned"])/len(df_03))
print(sum(df_04["attractions_mentioned"])/len(df_04))
print(sum(df_05["attractions_mentioned"])/len(df_05))
print(sum(df_06["attractions_mentioned"])/len(df_06))

"""### Input prep"""

final_data = pd.read_csv('abb_final_1.csv')

final_data.head()

df = final_data.loc[:,['number_of_reviews','top10_adj_keyword_rank','reviews_per_month','neighbourhood','room_type','price','minimum_nights','location_keyword_true','adj_keyword_true','attractions_mentioned','subways_mentioned','number_of_nearest_sub','distance_nearest_sub','distance_nearest_att','name_nearest_sub','name_nearest_att']]
df.columns

df=df_p
df

df.dtypes

df_ = copy.deepcopy(df)

cat_col = ['host_response_time','instant_bookable','cancellation_policy','room_type']

for col in cat_col:
    df_[col] = df_[col].astype('category')

df_.dtypes

df_.shape

train = df_.iloc[:3748,:]
test = df_.iloc[3748:,:]

Counter(train['popular_or_not'])

train_X = train.drop(columns= ['popular_or_not'])
Y = train['popular_or_not']
test_X = test.drop(columns= ['popular_or_not'])

stacked_df = train_X.append(test_X)

for col in stacked_df.columns:
    if str(stacked_df[col].dtype) =='category':
        stacked_df[col] = LabelEncoder().fit_transform(stacked_df[col])

for col in stacked_df.columns:
    print(stacked_df[col].dtype)

for col in cat_col :
    stacked_df[col] = stacked_df[col].astype('category')

for col in stacked_df.columns:
    print(stacked_df[col].dtype)

X = stacked_df[0:3748]
test_processed = stacked_df[3748:]

#check if shape[0] matches original
print("train shape: ", X.shape, "orignal: ", train.shape)
print("test shape: ", test_processed.shape, "original: ", test.shape)

Y = LabelEncoder().fit_transform(Y)

#sanity check to see numbers match and matching with previous counter to create target dictionary
print(Counter(train['popular_or_not']))
print(Counter(Y))
target_dict ={'popular': 1, 'not popular': 0}

#{'0-1': 1,
#         '1-2': 2,
#         '2-3': 3,
#         '3-4': 4,
#         '4-5': 5,
#         '5-above': 6,}

X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.20, random_state=0)
X_train.head()

df_cat = X.loc[:,cat_col]
embedded_cols = {n: len(col.cat.categories) for n,col in df_cat.items() if len(col.cat.categories) > 2}
embedded_cols

embedded_col_names = embedded_cols.keys()
len(X.columns) - len(embedded_cols) #number of numerical columns

embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]
embedding_sizes

class ShelterOutcomeDataset(Dataset):
    def __init__(self, X, Y, embedded_col_names):
        X = X.copy()
        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns
        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns
        self.y = Y
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        return self.X1[idx], self.X2[idx], self.y[idx]

#creating train and valid datasets
train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)
valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)

def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

class DeviceDataLoader():
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device
        
    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl: 
            yield to_device(b, self.device)

    def __len__(self):
        """Number of batches"""
        return len(self.dl)

device = get_default_device()
device

class ShelterOutcomeModel(nn.Module):
    def __init__(self, embedding_sizes, n_cont):
        super().__init__()
        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])
        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined
        self.n_emb, self.n_cont = n_emb, n_cont
        self.lin1 = nn.Linear(11, 56)
        self.lin2 = nn.Linear(56, 128)
        self.lin3 = nn.Linear(128, 56)
        self.lin4 = nn.Linear(56, 24)
        self.lin5 = nn.Linear(24, 2)
   #     self.lin6 = nn.Linear(128, 64)
     #   self.lin7 = nn.Linear(64, 2)
        self.bn1 = nn.BatchNorm1d(4)
        self.bn2 = nn.BatchNorm1d(56)
        self.bn3 = nn.BatchNorm1d(128)
        self.bn4 = nn.BatchNorm1d(56)
        self.bn5 = nn.BatchNorm1d(24)
       # self.bn6 = nn.BatchNorm1d(128)
       # self.bn7 = nn.BatchNorm1d(64)
        self.emb_drop = nn.Dropout(0.06)
        self.drops = nn.Dropout(0.03)
        

    def forward(self, x_cat, x_cont):
        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]
        x = torch.cat(x, 1)
        x = self.emb_drop(x)
        x2 = self.bn1(x_cont)
        x = torch.cat([x, x2], 1)

        x = F.relu(self.lin1(x))
       # x = self.drops(x)
        x = self.bn2(x)
        x = F.relu(self.lin2(x))
      #  x = self.drops(x)
        x = self.bn3(x)
        x = F.relu(self.lin3(x))
       # x = self.drops(x)
        x = self.bn4(x)
        x = F.relu(self.lin4(x))
      #  x = self.drops(x)
        x = self.bn5(x)
      #  x = F.relu(self.lin5(x))
       # x = self.drops(x)
      #  x = self.bn6(x)
      #  x = F.relu(self.lin6(x))
      #  x = self.drops(x)
      #  x = self.bn7(x)
      #  x = self.lin7(x)
        return x

model = ShelterOutcomeModel(embedding_sizes, 1)
to_device(model, device)

def get_optimizer(model, lr = 0.001, wd = 0.0):
    parameters = filter(lambda p: p.requires_grad, model.parameters())
    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)
    return optim

def train_model(model, optim, train_dl):
    model.train()
    total = 0
    sum_loss = 0
    for x1, x2, y in train_dl:
        batch = y.shape[0]
        output = model(x1, x2)
        loss = F.cross_entropy(output, y)   
        optim.zero_grad()
        loss.backward()
        optim.step()
        total += batch
        sum_loss += batch*(loss.item())
    return sum_loss/total

def val_loss(model, valid_dl):
    model.eval()
    total = 0
    sum_loss = 0
    correct = 0
    for x1, x2, y in valid_dl:
        current_batch_size = y.shape[0]
        out = model(x1, x2)
        loss = F.cross_entropy(out, y)
        sum_loss += current_batch_size*(loss.item())
        total += current_batch_size
        pred = torch.max(out, 1)[1]
        correct += (pred == y).float().sum().item()
    print("valid loss %.3f and accuracy %.3f" % (sum_loss/total, correct/total))
    return sum_loss/total, correct/total

def train_loop(model, epochs, lr=0.01, wd=0.0):
    optim = get_optimizer(model, lr = lr, wd = wd)
    for i in range(epochs): 
        loss = train_model(model, optim, train_dl)
        print("training loss: ", loss)
        val_loss(model, valid_dl)

batch_size = 500
train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)
valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)

train_dl = DeviceDataLoader(train_dl, device)
valid_dl = DeviceDataLoader(valid_dl, device)

train_loop(model, epochs=50, lr=0.05, wd=0.00001)

train_loop(model, epochs=100, lr=0.05, wd=0.00001)

test_ds = ShelterOutcomeDataset(test_processed, np.zeros(len(test_processed)), embedded_col_names)
test_dl = DataLoader(test_ds, batch_size=batch_size)

preds = []
with torch.no_grad():
    for x1,x2,y in test_dl:
        out = model(x1, x2)
        prob = F.softmax(out, dim=1)
        preds.append(prob)

final_probs = [item for sublist in preds for item in sublist]

len(final_probs)

target_dict

final_probs

print([float(t[0]) for t in final_probs])
print([float(t[1]) for t in final_probs])

len([float(t[0]) for t in final_probs])

test_processed

y_pred = []
for i in range(len([float(t[0]) for t in final_probs])):
    l1 = [float(t[0]) for t in final_probs][i]
    l2 = [float(t[1]) for t in final_probs][i]

    l = [l1,l2]
    if max(l) == l1:
        y_pred.append('1')
    elif max(l) ==l2:
        y_pred.append('0')

acc = 0
for i in range(len(y_pred)):
    if str(y_pred[i])==str(y_test[i]):
        acc+=1
acc/len(y_pred)

